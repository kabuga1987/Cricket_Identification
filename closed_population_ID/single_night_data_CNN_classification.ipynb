{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ab4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fca1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_helper import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6858309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \n",
    "    \n",
    "    def __init__(self, path_csv, path_samples):\n",
    "        \n",
    "        self.path_csv = path_csv\n",
    "        self.n1_trn = path_samples + \"n1_train.csv\"\n",
    "        self.n1_val = path_samples + \"n1_validation.csv\"\n",
    "        self.n1_tst = path_samples + \"n1_test.csv\"\n",
    "        self.n2_tst = path_samples + \"n2_test.csv\"\n",
    "        self.n3_tst = path_samples + \"n3_test.csv\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    def slice_data_frame(self, path):\n",
    "        \"\"\"Select Segment, and ID columns from a data frame\"\"\"\n",
    "        df = pd.read_csv(path)\n",
    "        return df[[\"Segment\", \"ID\"]]\n",
    "        \n",
    "    def sample_to_ID(self, df):\n",
    "        \"\"\"Map each sample to its ID given a data frame of samples and IDs\"\"\"\n",
    "        return dict([(sample, ID) for sample, ID in df.to_records(index = False)])\n",
    "    \n",
    "    def ID_to_samples(self, sample_2_ID_dict):\n",
    "        \"\"\"Group samples per ID\"\"\"\n",
    "        ID_2_samples = {}\n",
    "        for sample, ID in sample_2_ID_dict.items():\n",
    "            if ID not in ID_2_samples:ID_2_samples[ID] = [sample]\n",
    "            else:ID_2_samples[ID].append(sample)\n",
    "        return ID_2_samples\n",
    "    \n",
    "        \n",
    "    def hot_encod_labels(self, lst_samples, ID_2_samples_dict):\n",
    "        samp_2_numericID = dict([(a[j], i) for i, a in enumerate(ID_2_samples_dict.values()) for j in range(len(a))])\n",
    "        samples, IDs = zip(*[(samp, ID) for samp,ID in samp_2_numericID.items()])\n",
    "        IDs = to_categorical(IDs)\n",
    "        sample_2_ID = {samples[i]:IDs[i] for i in range(len(samples))}\n",
    "        samples, labels = zip(*[(sample, sample_2_ID[sample]) for sample in lst_samples])\n",
    "        return np.array(samples), np.array(labels)\n",
    "    \n",
    "    def get_samples(self, path): return pd.read_csv(path).Segment.values\n",
    "        \n",
    "    \n",
    "    def HotEncodeLabels(self):\n",
    "        \n",
    "        df = self.slice_data_frame(self.path_csv)\n",
    "        sample_2_ID = self.sample_to_ID(df)\n",
    "        ID_2_samples = self.ID_to_samples(sample_2_ID)\n",
    "        \n",
    "        n1_trn_s = self.get_samples(self.n1_trn)\n",
    "        n1_val_s = self.get_samples(self.n1_val)\n",
    "        n1_tst_s = self.get_samples(self.n1_tst)\n",
    "        n2_tst_s = self.get_samples(self.n2_tst)\n",
    "        n3_tst_s = self.get_samples(self.n3_tst)\n",
    "        \n",
    "        n1ts, n1ty = self.hot_encod_labels(n1_trn_s, ID_2_samples)\n",
    "        n1vs, n1vy = self.hot_encod_labels(n1_val_s, ID_2_samples)\n",
    "        n1es, n1ey = self.hot_encod_labels(n1_tst_s, ID_2_samples)\n",
    "        n2es, n2ey = self.hot_encod_labels(n2_tst_s, ID_2_samples)\n",
    "        n2es, n2ey = self.hot_encod_labels(n2_tst_s, ID_2_samples)\n",
    "        n3es, n3ey = self.hot_encod_labels(n3_tst_s, ID_2_samples)\n",
    "        \n",
    "        t1 = np.arange(len(n1_trn_s))\n",
    "        v1 = np.arange(len(n1_val_s))\n",
    "        e1 = np.arange(len(n1_tst_s))\n",
    "        t2 = np.arange(len(n2_tst_s))\n",
    "        t3 = np.arange(len(n3_tst_s))\n",
    "\n",
    "        np.random.shuffle(t1)\n",
    "        np.random.shuffle(v1)\n",
    "        np.random.shuffle(e1)\n",
    "        np.random.shuffle(t2)\n",
    "        np.random.shuffle(t3)\n",
    "        \n",
    "        return n1ts[t1], n1ty[t1], n1vs[v1], n1vy[v1], n1es[e1], n1ey[e1], n2es[t2], n2ey[t2], n3es[t3], n3ey[t3]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0e334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictions:\n",
    "    \n",
    "    def __init__(self, model,v1x, v1y, n1x, n1y, n2x, n2y, n3x, n3y,vgen, gen1, gen2, gen3, p_preds):\n",
    "        self.model = model\n",
    "        self.v1x = v1x\n",
    "        self.v1y = v1y\n",
    "        self.n1x = n1x\n",
    "        self.n1y = n1y\n",
    "        self.n2x = n2x\n",
    "        self.n2y = n2y\n",
    "        self.n3x = n3x\n",
    "        self.n3y = n3y\n",
    "        self.vgen = vgen\n",
    "        self.gen1 = gen1\n",
    "        self.gen2 = gen2\n",
    "        self.gen3 = gen3\n",
    "        self.p_preds = p_preds\n",
    "        \n",
    "        \n",
    "    def predict(self, model, samples, hotlabels, generator, night, verbose):\n",
    "        predictions = model.predict(generator, verbose = verbose)\n",
    "        observed = np.argmax(hotlabels, axis = 1)\n",
    "        predicted = np.argmax(predictions, axis = 1)\n",
    "        accuracy = np.where(observed == predicted)[0].shape[0]/observed.shape[0]\n",
    "        print(f\"accuracy for {night} = \", accuracy)\n",
    "        data = {\"samples\": samples, \"labels\": observed, \"predictions\": predicted}\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(self.p_preds + night + \".csv\")\n",
    "        return accuracy\n",
    "    \n",
    "    def execution(self):\n",
    "        av = self.predict(self.model, self.v1x, self.v1y, self.vgen,\"one_n1_val\", 0)\n",
    "        av = round(av, ndigits = 3)\n",
    "        print()\n",
    "        a1 = self.predict(self.model, self.n1x, self.n1y, self.gen1,\"one_n1_test\", 0 )\n",
    "        a1 = round(a1, ndigits = 3)         \n",
    "        print()\n",
    "        a2 = self.predict(self.model, self.n2x, self.n2y, self.gen2,\"one_n2_test\", 0 )\n",
    "        a2 = round(a2, ndigits = 3)\n",
    "        print()\n",
    "        a3 = self.predict(self.model, self.n3x, self.n3y, self.gen3,\"one_n3_test\", 0 )\n",
    "        a3 = round(a3, ndigits = 3)\n",
    "        \n",
    "        a = {\"accuracy\":[av, a1,a2,a3]}\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(a, orient = \"index\", columns = [\"night1v\",\"night1\", \"night2\", \"night3\"])\n",
    "        df = df.rename_axis(\"accuracy\")\n",
    "        df.to_csv(self.p_preds + \"one_clas_metrics.csv\")\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6a42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c58e355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 18:39:15.331060: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 18:39:16.114852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14632 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Training model has started\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 18:39:23.442304: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8600\n",
      "2024-10-10 18:39:24.439688: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Running ptxas --version returned 32512\n",
      "2024-10-10 18:39:24.500489: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 11s 428ms/step - loss: 1.5297 - accuracy: 0.4602 - val_loss: 1.0873 - val_accuracy: 0.3618 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.6283 - accuracy: 0.7841 - val_loss: 1.0963 - val_accuracy: 0.2321 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 2s 195ms/step - loss: 0.2248 - accuracy: 0.9398 - val_loss: 1.1243 - val_accuracy: 0.2526 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 2s 179ms/step - loss: 0.0526 - accuracy: 0.9818 - val_loss: 1.1850 - val_accuracy: 0.2526 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 2s 175ms/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 1.2851 - val_accuracy: 0.2526 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "7/7 [==============================] - 2s 181ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.4195 - val_accuracy: 0.2526 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 2s 178ms/step - loss: 9.9337e-04 - accuracy: 1.0000 - val_loss: 1.5349 - val_accuracy: 0.2526 - lr: 5.0000e-04\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 2s 195ms/step - loss: 5.0682e-04 - accuracy: 1.0000 - val_loss: 1.6585 - val_accuracy: 0.2526 - lr: 5.0000e-04\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 2s 180ms/step - loss: 3.3992e-04 - accuracy: 1.0000 - val_loss: 1.7891 - val_accuracy: 0.2526 - lr: 5.0000e-04\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 2s 193ms/step - loss: 2.5287e-04 - accuracy: 1.0000 - val_loss: 1.9264 - val_accuracy: 0.2526 - lr: 5.0000e-04\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0300e-04 - accuracy: 1.0000\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "7/7 [==============================] - 2s 184ms/step - loss: 2.0300e-04 - accuracy: 1.0000 - val_loss: 2.0696 - val_accuracy: 0.2526 - lr: 5.0000e-04\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 2s 193ms/step - loss: 1.7883e-04 - accuracy: 1.0000 - val_loss: 2.2164 - val_accuracy: 0.2526 - lr: 2.5000e-04\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 2s 178ms/step - loss: 1.6683e-04 - accuracy: 1.0000 - val_loss: 2.3716 - val_accuracy: 0.2526 - lr: 2.5000e-04\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 2s 176ms/step - loss: 1.5751e-04 - accuracy: 1.0000 - val_loss: 2.5331 - val_accuracy: 0.2526 - lr: 2.5000e-04\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 2s 161ms/step - loss: 1.4963e-04 - accuracy: 1.0000 - val_loss: 2.6988 - val_accuracy: 0.2526 - lr: 2.5000e-04\n",
      "Epoch 16/30\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3874e-04 - accuracy: 1.0000\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 1.4236e-04 - accuracy: 1.0000 - val_loss: 2.8665 - val_accuracy: 0.2526 - lr: 2.5000e-04\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 1.3761e-04 - accuracy: 1.0000 - val_loss: 3.0339 - val_accuracy: 0.2526 - lr: 1.2500e-04\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 2s 196ms/step - loss: 1.3466e-04 - accuracy: 1.0000 - val_loss: 3.1984 - val_accuracy: 0.2526 - lr: 1.2500e-04\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 2s 195ms/step - loss: 1.3201e-04 - accuracy: 1.0000 - val_loss: 3.3559 - val_accuracy: 0.2526 - lr: 1.2500e-04\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 2s 177ms/step - loss: 1.2929e-04 - accuracy: 1.0000 - val_loss: 3.5055 - val_accuracy: 0.2526 - lr: 1.2500e-04\n",
      "Epoch 21/30\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2742e-04 - accuracy: 1.0000\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "7/7 [==============================] - 2s 178ms/step - loss: 1.2684e-04 - accuracy: 1.0000 - val_loss: 3.6515 - val_accuracy: 0.2526 - lr: 1.2500e-04\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 2s 163ms/step - loss: 1.2457e-04 - accuracy: 1.0000 - val_loss: 3.7966 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 2s 161ms/step - loss: 1.2262e-04 - accuracy: 1.0000 - val_loss: 3.9406 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 2s 178ms/step - loss: 1.2064e-04 - accuracy: 1.0000 - val_loss: 4.0862 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 2s 192ms/step - loss: 1.1893e-04 - accuracy: 1.0000 - val_loss: 4.2315 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 2s 177ms/step - loss: 1.1697e-04 - accuracy: 1.0000 - val_loss: 4.3788 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 2s 183ms/step - loss: 1.1518e-04 - accuracy: 1.0000 - val_loss: 4.5316 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 2s 195ms/step - loss: 1.1348e-04 - accuracy: 1.0000 - val_loss: 4.6915 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 1.1182e-04 - accuracy: 1.0000 - val_loss: 4.8577 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 2s 195ms/step - loss: 1.1009e-04 - accuracy: 1.0000 - val_loss: 5.0249 - val_accuracy: 0.2526 - lr: 1.0000e-04\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "accuracy for one_n1_val =  0.36177474402730375\n",
      "\n",
      "accuracy for one_n1_test =  0.36610169491525424\n",
      "\n",
      "accuracy for one_n2_test =  0.46964064436183395\n",
      "\n",
      "accuracy for one_n3_test =  0.004761904761904762\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    p1 = \"../segment_index_extraction/segment_data.csv\"\n",
    "    p2 = \"train_val_test_segment_data/\"\n",
    "    p3 = \"classification_predictions/CNNs/\"\n",
    "     \n",
    "    PATH_ARRAY = \"../segment_spectrogram_mfcc_feature_extraction/spectrogram_arrays/\"\n",
    "    PATH_W = \"weights/n1_classification.h5\"\n",
    "    SHP = (128,173,1)\n",
    "    \n",
    "    LR = 1e-3\n",
    "    lr = 1e-4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    meanvar = True\n",
    "    \n",
    "    \n",
    "    encoder = LabelEncoder(p1,p2)\n",
    "    TIms,TLabs, VIms, VLabs, TeIms, TeLabs, N2Ims, N2Labs, N3Ims, N3Labs =  encoder.HotEncodeLabels()\n",
    "   \n",
    "    \n",
    "    Tgen = Generator(TIms, TLabs, SHP, BATCH_SIZE, PATH_ARRAY, meanvar)\n",
    "    Vgen = Generator(VIms, VLabs, SHP, BATCH_SIZE, PATH_ARRAY, meanvar)\n",
    "    Tegen = Generator(TeIms, TeLabs, SHP, BATCH_SIZE, PATH_ARRAY, meanvar)\n",
    "    n2gen = Generator(N2Ims, N2Labs, SHP, BATCH_SIZE, PATH_ARRAY, meanvar)\n",
    "    n3gen = Generator(N3Ims, N3Labs, SHP, BATCH_SIZE, PATH_ARRAY, meanvar)\n",
    "    \n",
    "   \n",
    "    architecture = ModelArchitecture(SHP)\n",
    "    trainer = Training()\n",
    "    trainer.architecture = architecture\n",
    "    \n",
    "    trainer.Tgenerator = Tgen\n",
    "    trainer.Vgenerator = Vgen\n",
    "\n",
    "    \n",
    "    \n",
    "    model = trainer.train(LR, lr, PATH_W, EPOCHS)\n",
    "    \n",
    "    predictor = Predictions(model,VIms,VLabs,TeIms,TeLabs,N2Ims,N2Labs,N3Ims,N3Labs,Vgen,Tegen,n2gen,n3gen, p3)\n",
    "    \n",
    "    \n",
    "    predictor.execution()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b366a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7d0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da9951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3213dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
